{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cda79/NLP-Week2-Text-Generation/blob/main/Chatbot_LLaMa_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "In this Colab Notebook, we are going to explore Llama-2 7B, a model fine-tuned for generating text & chatting.\n",
        "\n",
        "By the end of this tutorial, you'll be able to interact with this model and use it to generate conversational responses.\n",
        "\n",
        "Whether you're curious about chatbot technology or simply want to see a machine-generated response to a particular question, this notebook will serve as a comprehensive guide.\n",
        "\n",
        "## Workflow\n",
        "1. **Installations**: We'll begin by setting up our environment with the required libraries.\n",
        "2. **Prerequisites**: Ensure we have access to the Llama-2 7B model on Hugging Face.\n",
        "3. **Loading the Model & Tokenizer**: Retrieve the model and tokenizer for our session.\n",
        "4. **Creating the Llama Pipeline**: Prepare our model for generating responses.\n",
        "5. **Interacting with Llama**: Prompt the model for answers and explore its capabilities.\n",
        "\n",
        "Let's dive in!\n",
        "\n",
        "**First, change runtime to GPU.**\n",
        "\n",
        "\n",
        "You can play with Llama-2 7B Chat here: https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat"
      ],
      "metadata": {
        "id": "bWOx7lgoYHZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations\n",
        "\n",
        "Before we proceed, we need to ensure that the essential libraries are installed:\n",
        "- `Hugging Face Transformers`: Provides us with a straightforward way to use pre-trained models.\n",
        "- `PyTorch`: Serves as the backbone for deep learning operations.\n",
        "- `Accelerate`: Optimizes PyTorch operations, especially on GPU."
      ],
      "metadata": {
        "id": "DNrZtnUyYheg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch accelerate"
      ],
      "metadata": {
        "id": "aNTmMJIMYjiC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "367db373-6396-4bd6-a6d7-479532014782"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prerequisites\n",
        "\n",
        "To load our desired model, `meta-llama/Llama-2-7b-chat-hf`, we first need to authenticate ourselves on Hugging Face. This ensures we have the correct permissions to fetch the model.\n",
        "\n",
        "1. Gain access to the model on Hugging Face: [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf).\n",
        "2. Use the Hugging Face CLI to login and verify your authentication status.\n",
        "\n"
      ],
      "metadata": {
        "id": "BO2pb-EeZA95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "_6AfgF3_arYL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c6de771-76c9-426b-e67b-3c8967d0be0d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `colabtest` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `colabtest`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli whoami"
      ],
      "metadata": {
        "id": "6lD_oW1uavGp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e758161-ccd5-43e2-d438-6677ccc70ed5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m⚠️  Warning: 'huggingface-cli whoami' is deprecated. Use 'hf auth whoami' instead.\u001b[0m\n",
            "cda79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Model & Tokenizer\n",
        "\n",
        "Here, we are preparing our session by loading both the Llama model and its associated tokenizer.\n",
        "\n",
        "The tokenizer will help in converting our text prompts into a format that the model can understand and process."
      ],
      "metadata": {
        "id": "xmJHSjx4abta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model = \"meta-llama/Llama-2-7b-chat-hf\" # meta-llama/Llama-2-7b-hf\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)"
      ],
      "metadata": {
        "id": "jsBrtGpZYmcQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257,
          "referenced_widgets": [
            "53ea680d4890498cb19deb8c12b36c42",
            "d81513e2c2d94944891fc1783e6c4174",
            "9bbb6172ff2c4073aa2f305464d762fe",
            "b02253f627b94df68b3aa1c28f7d1914",
            "754f463fa22c4e339ff76694ed67ed02",
            "6e1c4abe029342488b83ef904362f70b",
            "6aea9414f7754d58b3800ed5e2e2be8c",
            "66b6d4ba083642de96ff06606718c0e2",
            "2391d8e39ce24453a7f3df528a1d1792",
            "60202120924243ea84cd8e5896c7b97e",
            "ecba8c45ed204099b06b50a9a78ba3ae",
            "bc3bbde77a9c4f14a34f7e7011d8eb3f",
            "693e54c3076640e2bc48d1cc4c698202",
            "e53e15ef2345497eb67d5b9f204e1d37",
            "c164670200a24d7cbbfbe98c8041136e",
            "3bae8d06bdbb45889185d2b287766e16",
            "0bd1ff78f183408fb17d06df0144a1a6",
            "d5c43fb473084022934f02ed3ab3b7ac",
            "7eb210075f2449fc9ff3c7b68de71aa0",
            "3c4ecbd6f7e94a23bba3ae657bcb703c",
            "2043dd7761d843e3ac971ae425ae5114",
            "f5e08cebd2dd4bd0947420298d7c867b",
            "7c1eda41777149e380ac289308d5b14b",
            "61536bd409534c6087a9c81f0a6e76af",
            "587a9e588aef4ca79ac969e1b9af5258",
            "5caa1a7700c644bb837d516e634c22aa",
            "74871ced96674f64958b8293612bf77f",
            "3b967db30f2e464e8e9da5fc84e10ff3",
            "23617f18bf6143aaa9f8318b6b5b1ee5",
            "e1dbce21d74c44119b5cd0760ceee588",
            "1d77854b1fde4dd6a6ee805103be0852",
            "2644b5b314c04887a262b1bb843a6fe7",
            "bcca9efcc736422781a3d34ee698a1c9",
            "30b965d039894b02897a6671e10223a8",
            "fbdca5c953754286aacdd8e49bd43d03",
            "082305b7ca154007882577c7fe4af7fc",
            "8a1fb8dd4f494149982add718ce11b6e",
            "404dd931d42d4552a90dfa04fc484198",
            "9779348266ab49939e54a5357c506da8",
            "4b1a5b684fd34fdc99e7117f6c238c23",
            "2bc2748f43ae4e8a99eead578281cb45",
            "d44c0abc97604d008b146f1fd4a9c7f9",
            "ce487724756e433c82aace1c76c21ac6",
            "bc32bc553057420785acb5d8dd82f1ca"
          ]
        },
        "outputId": "9a193618-6d01-40a2-f66d-0a208a3a396e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53ea680d4890498cb19deb8c12b36c42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc3bbde77a9c4f14a34f7e7011d8eb3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c1eda41777149e380ac289308d5b14b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30b965d039894b02897a6671e10223a8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the Llama Pipeline\n",
        "\n",
        "We'll set up a pipeline for text generation.\n",
        "\n",
        "This pipeline simplifies the process of feeding prompts to our model and receiving generated text as output.\n",
        "\n",
        "*Note*: This cell takes 2-3 minutes to run"
      ],
      "metadata": {
        "id": "cKHfNL76a6qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",  # LLM task\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "0AqJo1R_a9IM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472,
          "referenced_widgets": [
            "cafe6d9419a748c48dc7e4a153d49544",
            "849dd6fa36aa447097871e0279fa9918",
            "9c33e28259c44876bb8c2c610bb2e271",
            "5584d83b6ecc4a5aaf7da329427be551",
            "6ae3f9a0390149a6b48e574bb86b1aef",
            "35ccb719ccc84e0c92bdfbf363824195",
            "b54fa16e535a461e86022d4776f99592",
            "886266dcddb14f99827def7096640688",
            "4b723c61edb84002b7dc1b5f740d84d6",
            "61d9895876034a3e8bbc740a27371d5d",
            "ba5f1b6694224f37a98ada095d5ef319",
            "73bbfc97442e4358876c67cfe5862d7e",
            "c97600a43a1a45ea9609e898c6ee5adf",
            "5cf769ad16564555bb7646179b5a7fc1",
            "0cdc858b60ee486781fb603ea8874bd2",
            "b2085d6a151f418bbf66fea97b162404",
            "6562010312984fb19bc603e2d31a5acd",
            "9cc375e3435644bfb4dfa2c9eb3bfc29",
            "fee2ee9aea94428f9932b8a7f4eeccd3",
            "0c84d502e9e440c9ad47f93647c51a0b",
            "cd8aa0a29e9b4fb18f4e86a5e3093d50",
            "a3c708c8444b4eddb0847812e14d01cd",
            "1fb8363d77a444758d6c6ab5fad17f94",
            "dd7268806ea84fc9a3df2c645cef55ed",
            "a1d2107077ee44cdb395ce4592e12f77",
            "57da3f52e7eb4b85b00dc02fbf97a9ae",
            "7e62d76f70d746ae9e17f2e56fcf9ee0",
            "ab67fb01a9a94315ac14f8b8e633f506",
            "dd825248522f44caa433c6ae81a0c16c",
            "84b451756f534a63bba768dae4214ffe",
            "f73d6f435ae74eaca1dd5a0a34a5c6df",
            "6f10b7a4bf5242d5aefa7eba0a5cd194",
            "7417015beef8498a96e20c25214c9adc",
            "caa3d646c3b94a1ba02ba3c99a3daf57",
            "23b7c64910bf4326990ee1b719d78d28",
            "51263217a9024b76ac2caa034b9db7e6",
            "d65fdf6d0f754b079afebf4d08e3c61d",
            "f92dfd232942422bb106a79c7a33a512",
            "42649918ff8a46dc92b527e688d17e24",
            "e72cae30116a467b84214151767cbbac",
            "897fc366604046ddb84fe7671eac2841",
            "68188899ead84a58a705bc6e82ba4f47",
            "f2ecddc561fc4598a70a36df635134b6",
            "dfa5e6ade5234d21bbf446de9bccbb64",
            "1bd00e598891433fabddc2554d25b107",
            "7c32ebf978344d3e95700c16f256f56c",
            "28ddde6d56054a72b60f7c6ca12b18d0",
            "fd0270a978724752a1413d8115c03886",
            "c09e706d459543d8a3f5b3372eaee3d3",
            "c6e85ab22cdd4702bc778218edda999f",
            "6eef26d198ab4962aa59eb37effe98ca",
            "c2d40ea68d354a9cae5ec85c05e27650",
            "61648f348ca34c1b8a539bc47144e369",
            "d31062b13a6a4e4db21450eabe2d871b",
            "f413d9ce2bf649ae8b0672f716576be3",
            "8fed245d7806459ba911b2f540a27a02",
            "672e94f628cb4cb7b9f60840817f7107",
            "2b7cf72466ec4d36a5ffa9acc7ae6f12",
            "6efe739e1fd24bdd9507efea13802955",
            "e57c7ead58c74f95be530fce2891e6c1",
            "37cce4c47983480e867f946cc5415553",
            "2f41a4bf614040a9a17d6c10dd5e5890",
            "d968bc4b0ba04c858f9f61dc0d7ad50a",
            "224b8d1e6b2a4f65ae4266a96d9409d9",
            "edbceb6ca5fd41f1941908d968df0d9a",
            "05f39ff0139f475a88a3fc4f7c237944",
            "2c4cbceb6ba14cd196e2243169499969",
            "5b3d991188d048619a2bf93e4cc4412d",
            "038e7be0ab974c5395a70997b75c7c00",
            "24f8755ec80840e1bbb4435a672ebe6f",
            "840359399fa64c62948827e190246199",
            "bb04a01bb93549119d08631cc063fb0f",
            "394aa60b183b439ab090c874eae88d69",
            "a44127caff1447baa7bb50c7e0d20304",
            "ed78276dbc7e47a5bfa3ec9f662b5fd0",
            "f93dd7d900b84f3d9b41195ec28399e9",
            "1f614ba81db6444bb8114180f6d39bef"
          ]
        },
        "outputId": "7682fec7-82f4-41b4-cfab-883a5adc4583"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cafe6d9419a748c48dc7e4a153d49544"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73bbfc97442e4358876c67cfe5862d7e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1fb8363d77a444758d6c6ab5fad17f94"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "caa3d646c3b94a1ba02ba3c99a3daf57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1bd00e598891433fabddc2554d25b107"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fed245d7806459ba911b2f540a27a02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c4cbceb6ba14cd196e2243169499969"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Responses\n",
        "\n",
        "With everything set up, let's see how Llama responds to some sample queries."
      ],
      "metadata": {
        "id": "tlbsuGikcsB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llama_response(prompt: str) -> None:\n",
        "    \"\"\"\n",
        "    Generate a response from the Llama model.\n",
        "\n",
        "    Parameters:\n",
        "        prompt (str): The user's input/question for the model.\n",
        "\n",
        "    Returns:\n",
        "        None: Prints the model's response.\n",
        "    \"\"\"\n",
        "    sequences = llama_pipeline(\n",
        "        prompt,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_length=256,\n",
        "    )\n",
        "    print(\"Chatbot:\", sequences[0]['generated_text'])\n",
        "\n",
        "\n",
        "\n",
        "prompt = 'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n'\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "id": "m8RwW7Axcu9E",
        "outputId": "ced57907-a683-44d2-c203-4a2a85eb7756",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
            "\n",
            "I am a fan of crime drama and historical dramas. Is there anything else I might enjoy?\n",
            "\n",
            "Answer:\n",
            "\n",
            "If you enjoyed \"Breaking Bad\" and \"Band of Brothers,\" here are some other shows you might like:\n",
            "\n",
            "1. \"The Sopranos\" - This HBO series is a classic crime drama that explores the life of a New Jersey mob boss, Tony Soprano, as he navigates the criminal underworld and deals with personal and family issues.\n",
            "2. \"The Wire\" - This HBO series is a gritty and intense drama that explores the drug trade in Baltimore from multiple perspectives, including law enforcement, drug dealers, and politicians.\n",
            "3. \"Narcos\" - This Netflix series tells the true story of Pablo Escobar, the infamous Colombian drug lord, and the DEA agents who hunted him down.\n",
            "4. \"Peaky Blinders\" - This BBC series is a historical crime drama set in post-World War I England\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More Queries"
      ],
      "metadata": {
        "id": "7HSx6ctIdbW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"I'm a programmer and Python is my favorite language because of it's simple syntax and variety of applications I can build with it.\\\n",
        "Based on that, what language should I learn next?\\\n",
        "Give me 5 recommendations\"\"\"\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "id": "FoYglrLvmTN1",
        "outputId": "a3788ba5-1aba-4c70-ea85-b274d232c8a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: I'm a programmer and Python is my favorite language because of it's simple syntax and variety of applications I can build with it.Based on that, what language should I learn next?Give me 5 recommendations.\n",
            "\n",
            "As a programmer who enjoys the simplicity and versatility of Python, you may want to consider learning other languages that share similar qualities. Here are five language recommendations that you may find interesting:\n",
            "\n",
            "1. JavaScript: JavaScript is a popular language for web development, and is used by millions of websites around the world. It's known for its simplicity and flexibility, making it a great choice for building interactive web applications. Node.js, a JavaScript runtime, allows you to run JavaScript on the server-side, opening up even more possibilities for building scalable web applications.\n",
            "2. Ruby: Ruby is a dynamic language that's known for its simplicity and readability. It's a great choice for building web applications, and has a large and active community of developers who contribute to its ecosystem. Ruby on Rails, a popular web framework, makes it easy to build web applications quickly and efficiently.\n",
            "3. Swift: Swift is a relatively new language developed by Apple for building\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'How to learn fast?\\n'\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "id": "hk3QokHDdgx5",
        "outputId": "d7a64a72-631f-46a4-b8ac-88c28e5888b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: How to learn fast?\n",
            "\n",
            "There are several ways to learn quickly, including:\n",
            "\n",
            "1. Focus: Minimize distractions and focus on the task at hand.\n",
            "2. Repetition: Repeat what you are trying to learn multiple times to commit it to memory.\n",
            "3. Chunking: Break down complex information into smaller, more manageable chunks.\n",
            "4. Mnemonics: Use associations, acronyms, or rhymes to help you remember key information.\n",
            "5. Interleaving: Switch between different types of material to deepen your understanding and improve retention.\n",
            "6. Spaced repetition: Review material at increasingly longer intervals to help solidify it in your long-term memory.\n",
            "7. Active learning: Engage with the material through discussion, questions, or hands-on activities to help retain it.\n",
            "8. Sleep: Get enough sleep to help consolidate memories and improve learning.\n",
            "9. Exercise: Regular exercise has been shown to improve cognitive function and promote learning.\n",
            "10. Practice: The more you practice what you are trying to learn, the faster you will learn it.\n",
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'I love basketball. Do you have any recommendations of team sports I might like?\\n'\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "id": "HNCpOybWdc65",
        "outputId": "587bbffd-16f5-4936-f146-4b2b2b87cb8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: I love basketball. Do you have any recommendations of team sports I might like?\n",
            "\n",
            "Answer:\n",
            "If you enjoy basketball, you might also enjoy other fast-paced, high-scoring team sports like:\n",
            "\n",
            "1. Volleyball: Similar to basketball, volleyball is a fun and fast-paced game that involves a lot of jumping, running, and quick reflexes.\n",
            "2. Handball: Handball is a fast-paced game that combines elements of basketball, soccer, and gymnastics. It's played on a court with goals at each end, and players use their hands to score goals.\n",
            "3. Soccer: While soccer is a more physically demanding sport than basketball, it's still a great option for those who enjoy running and quick reflexes. Soccer is a team sport that involves a lot of strategy and skill, and it's played on a large field with goals at each end.\n",
            "4. Lacrosse: Lacrosse is a fast-paced game that involves a lot of running, dodging, and shooting. It's played with a small rubber ball and a long-handled stick\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'How to get rich?\\n'\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "id": "kzG2YoH4dil_",
        "outputId": "f67713bd-e37e-4411-f890-a0f0286db332",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: How to get rich?\n",
            "\n",
            "Getting rich is not an easy feat, but it is possible with the right mindset, strategy, and hard work. Here are some steps you can take to increase your chances of becoming wealthy:\n",
            "\n",
            "1. Set clear financial goals: Define what being \"rich\" means to you and set specific, measurable, achievable, relevant, and time-bound (SMART) financial goals.\n",
            "2. Live below your means: Spend less than you earn and save or invest the difference. Avoid buying things you don't need and focus on building wealth, not just spending money.\n",
            "3. Invest wisely: Invest your savings in assets that have a high potential for growth, such as stocks, real estate, or a small business. Do your research and seek professional advice to make informed investment decisions.\n",
            "4. Build multiple streams of income: Diversify your income sources to reduce financial risk. This could include starting a side business, investing in rental properties, or generating passive income through dividend-paying stocks or a blog.\n",
            "5. Minimize debt: Work on paying off high-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problems\n",
        "\n",
        "After 3-4 prompts, the model stops giving responses. It only outputs the user prompt.\n",
        "\n",
        "To keep talking to the model, you need to restart the notebook: `Runtime -> Restart Runtime` and run the notebook again..."
      ],
      "metadata": {
        "id": "xBfybeqxr3jT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make it conversational\n",
        "Let's create an interactive chat loop, where you can converse with the Llama model.\n",
        "\n",
        "Type your questions or comments, and see how the model responds!"
      ],
      "metadata": {
        "id": "De1MFB-xgavO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"bye\", \"quit\", \"exit\"]:\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "    get_llama_response(user_input)"
      ],
      "metadata": {
        "id": "a4xgy3NMgcwu",
        "outputId": "c9832e3f-cba6-45b9-9d7c-0770feb5a3ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: Whats up\n",
            "Chatbot: Whats up with the price of oil?\n",
            "\n",
            "The price of oil, particularly West Texas Intermediate (WTI), has been subject to significant volatility in recent years. There are several factors that contribute to the fluctuations in oil prices, including:\n",
            "\n",
            "1. Supply and demand: The balance between global oil supply and demand is the primary driver of oil prices. When demand for oil is strong and supply is limited, prices tend to rise. Conversely, when demand is weak and supply is abundant, prices tend to fall.\n",
            "2. Geopolitical events: Political instability, conflicts, and sanctions in oil-producing countries can disrupt oil supply lines and drive up prices. For example, tensions between the US and Iran have led to increased oil prices in recent years.\n",
            "3. OPEC (Organization of the Petroleum Exporting Countries) actions: OPEC is a cartel of oil-producing countries that coordinates the production and sale of oil on the global market. OPEC's actions, such as reducing oil output, can impact oil prices.\n",
            "4. Currency fluctuations: Changes in the value of the US doll\n",
            "You: Give me a famous shakespeare quote and then translate it into modern english.\n",
            "Chatbot: Give me a famous shakespeare quote and then translate it into modern english.\n",
            "Here is a famous quote from Shakespeare's Romeo and Juliet:\n",
            "\"What light through yonder window breaks? It is the east, and Juliet is the sun.\"\n",
            "\n",
            "In modern English, this quote would be:\n",
            "\n",
            "\"What light shines through that window? It's the east, and Juliet is like a bright new beginning.\"\n",
            "You: bye\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c5OW9VH2lVb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "Thanks to the Hugging Face Library, creating a pipeline to chat with llama 2 (or any other open-source LLM) is quite easy.\n",
        "\n",
        "But if you worked a lot with much larger models such as GPT-4, you need to adjust your expectations."
      ],
      "metadata": {
        "id": "s3lxvhWKqfFd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CWD3HFWlr2BE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
